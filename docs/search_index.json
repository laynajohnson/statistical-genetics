[["index.html", "Statistical Genetics Content Summary Introduction", " Statistical Genetics Content Summary Alayna Johnson 2025-03-27 Introduction In the following chapters, you will find concepts and examples related to statistical genetics explained. This breakdown is more directed towards those who are familiar with RStudio coding and statistical concepts, but who may not be as familiar with genetics or biology. You will learn different terms and methodologies in statistical genetics. The last section of this book is a Glossary containing terms referred to throughout the chapters. If you come across an unfamiliar term, refer to this section. Note that this site does not have all ideas and methods related to statistical genetics. This is only a small start, and there is much more to explore. References provided for definitions and concepts will be referenced in each section. Happy reading! "],["genome-wide-association-studies-gwas.html", " 1 Genome-Wide Association Studies (GWAS) 1.1 Introduction to GWAS 1.2 Simulating a GWAS 1.3 Marginal Regression 1.4 Visualizing GWAS Wrapping up", " 1 Genome-Wide Association Studies (GWAS) 1.1 Introduction to GWAS When looking at human DNA, we can see that between any two people, their genomes would be almost exactly identical. However, there are places where their DNA sequences differ. These are called genetic variants. Sometimes these variants mean nothing, other times they have been found to be linked to certain traits or health problems. The goal of a genome-wide association study (GWAS), is to determine which genetic variants are associated with a given trait of interest. 1.1.1 Genome Breakdown Before getting too far into GWAS, let’s take a moment to dive into some of the science-y details. I created a series of images that break down some of the different vocabulary from above and how they are related. Most humans have 23 pairs of chromosomes. Within each pair, you have one chromosome given from each parent. The chromosome for parent 1 is made up of a mixture of DNA from both of that parent’s parents and likewise for the chromosome for parent 2. We can also see that each chromosome is actually just an X-ish shape of DNA all bundled up. When we “unwrap” it, we can see some more levels of terms to remember. Figure 1.1: 23 pairs of chromosomes from parents (left) and Chromosome unwound DNA (right) These next two drawings show how the segments of DNA creates an individual’s genes and the nucleotides and alleles inside that create a genetic variant. So, within each long strand of DNA wrapped into the chromosome, there are segments that give us certain traits. These are called genes. The DNA and their genes are made up of four different nucleotide base pairs in between the sugar-phosphate backbone of the DNA (the outside lines). The four nucleotide bases are: adenine (A), thymine (T), cytosine (C), and guanine (G). In the vocab section, I mentioned that these are called alleles and they have specific pairings. A will always be with T just as C will always be with G. We can see in these visualizations that we can zoom into some segement of DNA and see a SNV where there are two possible alleles. This is called biallelic. Figure 1.2: Some gene segment from DNA sequence (left) and genetic variant (right) Now that we can better understand some genomic structures, let’s get back into explaining GWAS. 1.1.2 A small example Below is an example of a small study of alleles that can be different at some place along the DNA sequence. We can call this a biallelic genetic variant. The goal of this GWAS study is to see if the pairings of alleles at certain positions have some effect on the trait. We know from before that A and T pair as well as C and G. This means we really only need to list one nucleotide base in our data given the other is known. person ID parent 1 allele parent 2 allele trait 10005 A A 60 10006 A G 67 10007 G A 68 10008 A A 62 Instead of having this more complex table, if we know G is the minor allele in this position we can recode the table. Instead of two columns for A and G, we can have one column combining the two where G is 1 and A is 0. It doesn’t matter the order anyways, so this transformation is fine. Here is what that would look like: person ID alleles trait 10005 0 60 10006 1 67 10007 1 68 10008 0 62 The typical GWAS model looks like this. Think about what would go into the variables below from our small example. \\[ E[y \\mid x] = \\beta_0 + \\beta_1 x \\] We are able to use this model easily as our data has only one position of interest, or SNP. But what happens when we have a data set where there are more columns than there are rows, \\((p&gt;n)\\)? The answer here is that we run into an issue of big data. We must have the same or more observations as we do coefficients in order to effectively run a regression model. When we have too many coefficients and not a sufficient number of observations, we will get NA values. The DNA sequence of a single person consists of an estimated 3 billion nucleotides. While a GWAS will not typically measure every single position, they can still be hundreds of thousands to billions of columns. The “study” we did above was a very reduced example of what a GWAS could look like. So, how can we estimate the effect of some genetic variant on our trait of interest if we cannot use one giant regression model? 1.2 Simulating a GWAS Using the rbinom function, we can randomly generate data from a binomial distribution. We choose a size of \\(n = 100\\) people and a probability of having the minor allele as \\(p = 0.1\\). The last argument in the function tells us the number of trials, or in this case, each person is randomly assigned to have either 0, 1, or 2 minor alleles. set.seed(494) # for reproducible random number generation snp &lt;- rbinom(n = 100, size = 2, p = 0.1) # 100 people, MAF = 0.1 print(snp) ## [1] 0 0 0 0 0 0 0 0 1 1 0 1 0 1 1 1 0 0 0 0 ## [21] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 ## [41] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 ## [61] 0 0 1 0 1 1 0 0 0 0 1 0 0 0 0 2 0 0 0 0 ## [81] 0 0 0 1 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 0 We know from before that we actually would see way more than just one genetic variant for \\(n\\) number of people. So, we can extend the above rbinom function into our own function in which we can choose the number of genetic variants to simulate per person # function doing the same as the one before, where we get to choose the number of people in the study and the minor allele frequency (MAF) do_one &lt;- function(n_ppl, MAF){ snp &lt;- rbinom(n = n_ppl, size = 2, p = MAF) return(snp) } # combining the previous function with the `replicate` function to replicate 1000 times to create 1000 variants set.seed(494) snps &lt;- replicate(1000, do_one(n_ppl = 100, MAF = 0.1)) This new snps object has 100 people each with 1000 variants. We can use the functions nrow and ncol to see that data is \\((p&gt;n)\\). nrow(snps) ## [1] 100 ncol(snps) ## [1] 1000 Now, we can create some numeric trait using the rnorm function and combine the data with our SNPs from before using cbind. set.seed(494) # set seed for reproducibility y &lt;- rnorm(100, mean = 65, sd = 3) # generate trait print(y) ## [1] 59.62481 63.88034 63.02679 62.50892 ## [5] 70.83525 67.50514 63.61816 69.64214 ## [9] 64.97347 65.47989 63.13887 63.07716 ## [13] 63.74931 64.70136 65.07071 58.33367 ## [17] 61.91952 64.85250 65.66969 62.40541 ## [21] 62.04025 61.84794 64.28388 65.04385 ## [25] 61.42188 62.55285 59.29327 67.46579 ## [29] 61.65637 67.67236 66.09086 70.02120 ## [33] 68.01750 60.73839 58.42093 67.99430 ## [37] 65.89968 64.39152 65.65709 66.54789 ## [41] 65.00309 65.34142 63.41219 68.68026 ## [45] 61.68235 61.18094 68.73800 69.94243 ## [49] 66.55700 60.87955 66.43440 56.08365 ## [53] 69.62706 68.09680 68.85878 64.92834 ## [57] 63.05479 65.98005 64.87043 64.69823 ## [61] 65.76869 67.50849 65.20761 62.59497 ## [65] 68.12195 67.62197 66.63407 66.00194 ## [69] 62.77750 63.89318 63.51581 71.79953 ## [73] 65.50906 68.08233 68.99696 64.76795 ## [77] 69.99623 66.88639 65.01119 65.69350 ## [81] 64.53200 64.69204 65.19411 64.11513 ## [85] 63.19411 62.63277 62.99986 64.56554 ## [89] 67.31328 62.24985 65.15231 61.25394 ## [93] 67.23057 65.94068 64.43938 69.38935 ## [97] 66.89983 65.07994 63.90870 60.93687 # creating a single data frame for snps and trait dat &lt;- as.data.frame(cbind(y, snps)) With our SNPs and trait data together, we can perform a simulated GWAS study. But, we need another tool to help us solve this \\((p&gt;n)\\) problem that occurs from trying to use one giant regression model. This will be explained next. 1.3 Marginal Regression For the extent of this text, we will not be performing an actual GWAS study with real data. However, we will explain the idea of marginal regression and what it has to do with data that is \\((p&gt;n)\\). With our simulation in the last section, if we were to try to fit a multiple linear regression model with the data we would get NA values for many of the coefficients. We cannot estimate more coefficients than we have observations. For our simulation this means that we cannot fit this model as there are 100 observations (people) and 1000 variants (coefficients). Marginal regression is a technique of dealing with \\((p&gt;n)\\) data. We can fit individual models for each variant and check which are significantly related with our trait of interest. \\[ \\begin{align} E[y \\mid x_j] = \\beta_0 + \\beta_{1j} x_j, \\qquad y &amp;= \\text{trait of interest}\\\\ x_j &amp;= \\text{number of minor alleles at position }j \\end{align} \\] The model above represents what one marginal regression model looks like. We would repeatedly fit a model like this for all positions \\(j = 1, 2, \\dots m\\). To find out if our SNPs are associated with our trait of interest, we also perform hypothesis testing for each model. For each model, our null hypothesis, \\(H_0\\), is that there is no relationship between the genetic variant and our trait of interest. This means the alternative, \\(H_A\\), is that there is a relationship present. Using our example from before, say we fit 1000 models and perform 1000 hypothesis tests. When performing just one test, we would typically use a significance threshold of \\(\\alpha = 0.05\\), or, \\(p&lt;0.05\\). As a reminder, we choose a value for \\(\\alpha\\) based on how “comfortable” we are with making a type 1 error. A type 1 error happens when we reject \\(H_0\\) and say there is a relationship between the SNP and trait of interest. If we only conduct one hypothesis test, this means the probability of making a type 1 error is \\(P(\\text{T1E}) = 0.05\\). As we increase the number of tests, the probability of making at least one type 1 error increases. But we are not actually comfortable with concluding there is a relationship when there is truly not one so many times. This is the idea of multiple testing. There are many ways to adjust for this by making our significance threshold smaller. 1.3.1 Multiple testing One way to adjust for multiple testing is to use the Bonferroni Correction. Say we still want the probability of at least one type 1 error at less than 5% with the 1000 tests we would have to perform with our simulation data. We can find this significance threshold by dividing the probability by the number of hypothesis tests we performed. This makes the significance threshold pretty small (\\(5 \\ast 10^{-5}\\)). But a typical GWAS will involve hundreds of thousands or even billions of SNPs. So our threshold can get pretty tiny for this many hypothesis tests (e.g. \\(5 \\ast 10^{-11}\\)). However, this might be too conservative and lead us to make type 2 errors. This means that our threshold might actually be too small which could lead us to not reject the null when we should have (found no association where we should have). We could also adjust for multiple testing by using a simulation-based approach. This solution is way more computationally expensive than a Bonferroni Correction. Steps to this process can look something like this: Simulate a null trait (i.e., a trait that is not associated with any of the SNPs) Run GWAS to test the association between this simulated null trait and each of the SNPs in our dataset. Record the smallest p-value from this GWAS. Repeat steps 1–3 many (e.g., 100, 1000) times. Look at the p-values you saved from those simulation replicates. Sort them from smallest to largest and find the lowest 5th percentile (i.e., the point at which 5% of the p-values are smaller than that number). The positives of this approach is that our p-value threshold will most likely be less conservative than if we used the Bonferroni Correction instead. 1.4 Visualizing GWAS Since our studies may have many many SNPs to test significance for, it can be hard to visualize. If we just take and plot our p-values vs each SNP’s position in a scatter plot, it won’t look very nice. There are too many values crowing around each other. Visualizing data always helps, and there is a fix for this issue. We can create a Manhattan Plot with the manhattan() function in the qqman package to visualize our significance results. Below is an image of an example Manhattan plot to give an idea of what our p-values look like for each SNP. Figure 1.3: Manhattan plot example Another type of visualization we can use as a diagnostic tool are called QQ plots. These plots allow us to compare the results from our study to what we would see if no SNPs were associated with our disease of interest. We would expect that not all of the SNPs in our study are associated with the trait, so most should fall on the \\(y = x\\) line. If there are some SNPs that we find are associated with the trait of interest, we would visually see some points above the diagonal \\(y = x\\). Figure 1.4: QQ plot example Wrapping up With that, we have come to the end of this chapter on genome-wide association studies (GWAS). You now have many of the concepts you need to understand this methodology and attempt to perform some studies yourself. "],["genetic-ancestry.html", " 2 Genetic Ancestry 2.1 Describing Human Populations 2.2 Principal Component Analysis 2.3 Confounding Variables Wrapping Up", " 2 Genetic Ancestry What is ancestry, and what can genetic data tell us about it? There are several ways that one can think about genetic ancestry. In these chapters, we will consider genetic ancestry to loosely mean the genetic relationship between you and your ancestors. An important idea to remember throughout this chapter is genetic similarity. If you have ever taken a genetic test through a company like 23andMe or Ancestry.com you are most likely familiar with this concept. When you take one of these tests by sending in some of your DNA, these companies are reporting back to you how similar your DNA is compared to different populations. The concept of different populations comes up a lot in these contexts. As with all scientific research, there are some ethical issues surrounding how researchers classify populations in genetic studies. We will get into this next. 2.1 Describing Human Populations When a researcher is working with genetic data, they will likely use some sort of population descriptor to attempt to capture complex differences in human genetics. When you look at the results of your personal DNA test, you will likely see your ancestry broken down into smaller percentages of regions or peoples that contribute to 100% you. The way these regions are described brings with it some questions you may have never thought of before: How are regions defined? Are political borders used in every case? What is the genetic “line” between being reported as native to one area versus another geography close by? How specific can this get? So, what are the recommended ways to describe human populations? To start, you should never use race as a descriptor of genetic variation. Race is a concept created by white people in the past to other different peoples. There is no reason to use these arbitrary descriptors in a genetic study. Different populations do not exist as discrete genetic clusters, but instead as a continuum across space and time. Assigning constricting labels to different populations can be harmful, especially if you are not conscious of potential impacts of your chosen terminology. Think about if there are different ways to classify populations related environmental factors rather than the people themselves. Genetic similarity, geography, and indigeneity are some top choices provided by the National Academies of Sciences. Explaining the choices for using specific terminology to describe human populations. Being aware of cultures and incorporating their thoughts into your choices. Maybe you also need multiple labels, but it is important to remain consistent. Do your research on what the current recommendations are for the type of genetic study you are performing and what different institutions and organizations encourage. These can change with time, so make sure to check with any and every study. 2.1.1 Population Admixture As our world has become so interconnected between geographies and people’s from long ago to the present, many of us have DNA inherited from multiple populations. The general term for this is population admixture. There is not really any population that is completely homogeneous. Populations in the Americas tend to be more admixed due to history of world discovery, slavery, and immigration as compared to other continents. In the United States especially, there are many more people with ancestors all over the world. That is not to say the Americas are the only place with admixed populations. Those with European ancestry, for example, can be more similar to each other than to individuals with ancestry from a different geographical/continental area, but there are still admixed populations here when we focus our scale to Europe only. When doing genetic research, you should keep this concept in mind as you collect, process, and report your data. Think about how ancestry is related to your study and which ones you are including to help guide your naming conventions and disclaimers on population differences. One way to go about this is to think of social classifications of population and trying to break those down. What would that look like? Ancestry in Genetics Studies When we conduct a genetic study, we want to be aware of the ancestry of our samples. Things like genetic similarity and continental geography come to mind. It is important to correct for these population structures built into our data so we can have the most accurate results. One method we can use to account for population structures in genetic data is by performing Principal Component Analysis. We can also use PCA on admixed ancestry. We will go into what PCA is and how to apply it to genetic data next. 2.2 Principal Component Analysis If you have taken a class in linear algebra or machine learning, you may be familiar with the method of Principal Component Analysis (PCA). We use PCA as a technique to reduce the dimensionality of correlated variables while keeping as much variation as possible. We will be left with principal components (PC’s) as weighted combinations of these variables that explain the most variation in our data. Essentially, instead of having thousands of SNPs representing genetic variation, this method will represent genetic variation in fewer dimensions made up of the original data. The first few PC’s explain the most variation in our data. Here is an example of what the PC’s we create from this method could look like: \\[\\begin{align} \\text{PC}1 &amp;= a{11}x_1 + a_{12}x_2 + \\dots + a_{1p}x_p \\\\ \\text{PC}2 &amp;= a{21}x_1 + a_{22}x_2 + \\dots + a_{2p}x_p \\\\ &amp;\\vdots \\\\ \\text{PC}n &amp;= a{n1}x_1 + a_{n2}x_2 + \\dots + a_{np}x_p \\end{align}\\] where \\[\\begin{align} &amp; x_1, x_2, \\dots, x_p \\text{ are the original SNPs and} \\\\ &amp; a_{ij} \\text{ are the weights of each SNP towards a PC} \\end{align}\\] The \\(a_n\\) values represent the weights of each SNP that goes into each principal component. We call these the loadings. They are essentially telling you how important each SNP is to the PC. The \\(a_N\\) values combined with the \\(x_1, x2, \\dots\\) values create our PCs, and these values we call the scores. 2.2.1 Genetic Data PCA We can use PCA with genetic data in order to correct for stratification from things like population (which is what we will be looking at with an example below). Say there is some set of SNPs that have very different MAF between populations. These features are most likely correlated to each other and stratify the data by representing the difference between populations and will contribute most to the top PCs. Let’s simulate some genetic data to run PCA on and break down some of the concepts briefly mentioned above. We will use some similar simulation code to the last chapter on GWAS. library(tidyverse) library(broom) library(GGally) Simulating data # simulate a genetic variant with freq. depending on which population the person belongs to sim_data_onevar &lt;- function(pop, maf){ snp &lt;- rbinom(n = length(pop), size = 2, p = maf[pop]) return(snp) } # create dataset with two types of SNPs popidx &lt;- rep(1:2, each = 400) maf1 &lt;- c(0, 0.8) # type 1: non-existent in pop 1, frequent in pop2 (different btwn pops) maf2 &lt;- c(0.5, 0.3) # type 2: higher frequency in pop 1 and lower in pop 2 (different btwn pops) maf3 &lt;- c(0.1, 0.11) # type 3: similar freq in pops 1 and 2 set.seed(494) # arguments include number of snps to create, and the population and maf for each pop we created before snps1 &lt;- replicate(3, sim_data_onevar(pop = popidx, maf = maf1)) snps2 &lt;- replicate(4, sim_data_onevar(pop = popidx, maf = maf2)) snps3 &lt;- replicate(13, sim_data_onevar(pop = popidx, maf = maf3)) pcadata &lt;- cbind(popidx, snps1, snps2, snps3) %&gt;% as.data.frame() names(pcadata) &lt;- c(&#39;population&#39;, paste0(&#39;SNP&#39;, 1:(ncol(pcadata)-1))) # simulating a trait on snp 4 in our data n &lt;- 800 # number of individuals trait &lt;- 2 * pcadata[,&#39;SNP4&#39;] + rnorm(n, 0, 1) # y = 2 * x + e trait &lt;- as.data.frame(trait) # joining snps and trait pcadata &lt;- as.data.frame(cbind(trait, pcadata)) Calculating the MAF for each SNP in our data summarized by population. We create a function that will add the minor alleles and divide by 2x the number of people we have in the data. get_MAF &lt;- function(snp){ sum(snp)/(2*length(snp)) } # get observed allele frequency for each population pcadata %&gt;% group_by(population) %&gt;% summarize_all(get_MAF) ## # A tibble: 2 × 22 ## population trait SNP1 SNP2 SNP3 SNP4 ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1.03 0 0 0 0.515 ## 2 2 0.560 0.811 0.785 0.821 0.286 ## # ℹ 16 more variables: SNP5 &lt;dbl&gt;, ## # SNP6 &lt;dbl&gt;, SNP7 &lt;dbl&gt;, SNP8 &lt;dbl&gt;, ## # SNP9 &lt;dbl&gt;, SNP10 &lt;dbl&gt;, SNP11 &lt;dbl&gt;, ## # SNP12 &lt;dbl&gt;, SNP13 &lt;dbl&gt;, SNP14 &lt;dbl&gt;, ## # SNP15 &lt;dbl&gt;, SNP16 &lt;dbl&gt;, SNP17 &lt;dbl&gt;, ## # SNP18 &lt;dbl&gt;, SNP19 &lt;dbl&gt;, SNP20 &lt;dbl&gt; In order to run PCA, we need to have only the genotype information from our data. This means removing the population and trait variables. Then we will perform PCA on our remaining data. genotype &lt;- pcadata %&gt;% select(-population, -trait) # performing pca pca_out &lt;- prcomp(genotype, center = TRUE, scale = TRUE) # storing loadings and scores from pca pca_loadings &lt;- pca_out$rotation pca_scores &lt;- pca_out$x Visualizing PCA Let’s take a look at what the components of our PCs look like. Score Plot pca_scores %&gt;% as.data.frame() %&gt;% # convert pca_scores into a data frame for plotting mutate(population = as.factor(pcadata$population)) %&gt;% # add the population labels ggplot(aes(x = PC1, y = PC2, color = population)) + # then plot geom_point() + theme_minimal()+ scale_color_brewer(palette = &#39;Dark2&#39;) The scores above represent the new values for the individuals in our study along the PC1 and PC2 axes. In our plot, we colored the points by population. Before we performed PCA, we took out our population variable, yet this graph shows how our data is still separated into two populations through this method. We can also use a kind of visualization called parallel coordinates plot. This plot is another way to view how the PCs capture our populations by showing the scores for all PCs at one time. As we learned from our score plot, we see PC1 and PC2 capture population membership, while later PCs do not. # parallel coordinates plot pca_scores %&gt;% as.data.frame() %&gt;% mutate(population = as.factor(pcadata$population)) %&gt;% ggparcoord(columns = 1:20, groupColumn = &#39;population&#39;, alpha = 0.2) + theme_minimal() + scale_color_brewer(palette = &#39;Dark2&#39;) Remembering from before, loadings show us how much each SNP contributes to a PC. We can view this PC by PC. We will look at plots for for PC1 to PC4. library(reshape2) pca_loadings[, 1:4] %&gt;% melt() %&gt;% ggplot(aes(x = Var1, y = value, fill = Var1)) + geom_bar(stat = &quot;identity&quot;) + facet_wrap(~ Var2) + labs(y = &quot;loadings&quot;, x = &quot;original features&quot;, fill = &quot;original features&quot;) + theme_minimal() + theme(axis.text.x=element_blank(), axis.ticks.x = element_blank()) Scree plot When we explained what PCA is, we mentioned this idea that we are performing dimension reduction to be left with components that explain the most variation in our data. We also have a way of visualizing the proportion of variance explained by each PC. The plot we will make is called a scree plot. # extract variance of each PC pca_var &lt;- (pca_out$sdev)^2 # calculate proportion of variance explained total_var &lt;- sum(pca_var) pve &lt;- pca_var/total_var # creating scree plot pve %&gt;% as.data.frame() %&gt;% mutate(index = seq_len(length(pca_var))) %&gt;% ggplot(aes(x = index, y = pve)) + geom_point() + geom_line() + labs(x = &#39;Principal Component&#39;, y = &#39;Percent of Variance Explained&#39;) + theme_minimal() In our plot, we can see that our first PC has the highest proportion of variation compared to the rest. PC1 explains around 16% of the variance and then there is a drop for the rest of the PCs. We mentioned before that our method will have the first PCs explain the most variance, and this holds true for the PCA we ran on our simulated genetic data as well. Another thing we can understand from a scree plot is how many PCs are sufficient to capture population structure. From our plot, we can see our first PC captures a lot more variance as compared to PC2-PC20. The drop off in the plot lets us know that PC1 is sufficient and we would not need to adjust for others to correct our analysis for population structure. 2.2.2 GWAS and PCA In the last chapter, we did not run GWAS on our simulated chapter. With this chapter, we will get to see what this looks like in practice. Before adjusting for PCs With this first GWAS example, we provide code that looks like just a normal GWAS where we use marginal regression for each of our 20 SNPs on the trait we simulated. What we would want to see is that the most significant SNP in predicting our trait is the one we simulated the trait on (SNP4). # empty vector to store p-values pvals &lt;- c() # loop through the 20 SNPs for(i in 1:20){ dat &lt;- pcadata %&gt;% select(trait, paste0(&#39;SNP&#39;,i)) # pull out just the trait and SNP of interest mod &lt;- lm(trait ~ ., data = dat) # regress trait on everything else (.), which is just SNP i in this case pvals[i] &lt;- summary(mod)$coef[2,4] # pull out the p-value for the slope } # plot -log10 pvalues data.frame(p = pvals, SNP = 1:20) %&gt;% ggplot(aes(y = -log10(p), x = SNP)) + geom_point() + theme_minimal() + geom_hline(yintercept=8, color = &quot;firebrick&quot;, alpha = 0.5)+ ggtitle(&#39;Unadjusted Analysis&#39;) We see with our plot that our SNP4 does indeed have the smallest p-value. But we also see that our first three SNPs seem significant as well at around 15 on the \\(-log10\\) scale. The horizontal line on the graph represents the agreed upon “typical” level of significance for a GWAS study. Note: the \\(-log10(p)\\) notation means that we are transforming the visual of our p-value (p) to put more emphasis on those with the lowest values (log scale). You may notice that it is \\(-log\\) which means that on our graph we are looking for the highest value instead of the lowest value for which SNP is most significant. (think of it like \\((p)^{-5}\\) now looks more like \\((p)^5\\)). Adjusting for PCs # empty vector to store p-values pvals &lt;- c() # loop through the 15 SNPs for(i in 1:20){ dat &lt;- pcadata %&gt;% select(trait, paste0(&#39;SNP&#39;,i)) %&gt;% # pull out just the trait and SNP of interest mutate(PC1 = pca_scores[,1]) # add the scores for PC1 mod &lt;- lm(trait ~ ., data = dat) # regress trait on everything else (.), which is SNP i and PC1 pvals[i] &lt;- summary(mod)$coef[2,4] # pull out the p-value for the slope } # plot -log10 pvalues data.frame(p = pvals, SNP = 1:20) %&gt;% ggplot(aes(y = -log10(p), x = SNP)) + geom_point() + geom_hline(yintercept=8, color = &quot;firebrick&quot;, alpha = 0.5)+ theme_minimal() + ggtitle(&#39;Adjusted Analysis&#39;) After adjusting for our first PC, which explains much of the difference between the two populations, the significance of our first three SNPs goes down. Looking at where the p-values are compared to our reference line, they are below what we would consider significant. What could be the reason for the difference between the two models? We will talk about it next. 2.3 Confounding Variables A term you may have come across in statistics or statistics related courses is the idea of confounding. In some instances, all you may know about a confounding variable is that you should adjust for it. But what does it mean to adjust for a confounding variable and how do we really know if a variable is confounding or not? Nearly all genetics studies are observational studies. This means that the researcher simply collects genetic data or uses existing data and views the trends present. Mainly we use observational studies because there are plenty of ethical concerns that come with experimental human genetic studies. Just like you need to be sure to account for different variables in an experiment that can affect your collection and results, you also need to be aware of variables that interact with observational studies. A term you may be less familiar is Directed Acyclic Graph (DAG). These are graphs where the edges are directed, or have arrows explaining the direction of how variables are related or “cause” each other. So, if we think of a basic linear model, we can draw a DAG that shows how we think our variables in the model are related. Figure 2.1: QQ plot example There are three criteria for confounding to think about when considering variables to add to your model. Confounding variable \\(Z\\) must be associated (“cause”) our predictor \\(X\\). Confounding variable \\(Z\\) must be associated (“cause”) our outcome \\(Y\\). Confounding variable \\(Z\\) cannot be on the causal pathway between predictor \\(X\\) and outcome \\(Y\\). Figure 2.2: QQ plot example The reason we are talking about confounding at all is because of the population variable. We adjusted for populationwith our PCA method. It fits all three criteria for the SNPs that we simulated with different MAF between populations 2.3.1 Omitted Variable Bias Another set of terms you may or may not be familiar with are bias and variance. Bias refers to how far away we are from true value while variance refers to how much our results differ from attempt to attempt. Figure 2.3: QQ plot example When we run linear regression (OLS, ordinary least squares), we are finding the \\(\\beta\\) coefficients for each variable in our model. So, this means for OLS, our estimator is \\(\\beta\\). An estimator is unbiased if its expected value is equal its true value For OLS this looks like: \\[ E[\\hat\\beta] = \\beta \\] Lets prove that the OLS estimator is unbiased. We want to fit the linear regression model \\[ E[y|x] = X\\beta \\] We can derive the estimator for \\(\\beta\\) with some linear algebra (check here for how we get this). \\[ \\hat\\beta = (X^TX)^{-1}X^Ty \\] The last bit of information we need to start working on this proof that \\(E[\\hat\\beta]= \\beta\\) (OLS estimator is unbiased) is to know that we fit the right model. Assume we did this. The model we are building is \\(\\hat y = X\\hat\\beta\\). \\[ y = X\\beta + \\epsilon\\\\ \\text{where, } E[\\epsilon]=0 \\] Finally, here is what this proof looks like: \\[\\begin{align} E[\\hat\\beta] &amp;= E[(X^TX)^{-1}X^Ty]\\\\ &amp; = (X^TX)^{-1}X^T\\space E[y]\\\\ &amp; = (X^TX)^{-1}X^T\\space E[X\\beta + \\epsilon]\\\\ &amp; = (X^TX)^{-1}X^TX\\space \\beta \\space + \\space E[\\epsilon]\\\\ &amp; =\\beta\\\\ E[\\hat\\beta]&amp; = \\beta \\end{align}\\] Wow, the OLS estimator is unbiased when we fit the correct model! But what happens if we do not fit the correct model? Let’s do another example. Say it is true that \\(y = X\\beta + Z\\gamma +\\epsilon\\), where again \\(E[\\epsilon] = 0\\). Even though this is the truth, we don’t know this. So we instead fit the same model from before: \\(\\hat y = X\\hat\\beta\\). We use the same \\(\\hat\\beta\\) derived value from earlier. \\[\\begin{align} E[\\hat\\beta] &amp;= E[(X^TX)^{-1}X^Ty]\\\\ &amp; = (X^TX)^{-1}X^T\\space E[y]\\\\ &amp; = (X^TX)^{-1}X^T\\space E[X\\beta + Z\\gamma + \\epsilon]\\\\ &amp; = (X^TX)^{-1}X^T\\space (E[X\\beta] + E[Z\\gamma] + E[\\epsilon])\\\\ &amp; = (X^TX)^{-1}X^T\\space (X\\beta + Z\\gamma + 0)\\\\ &amp; = ((X^TX)^{-1}X^T X\\beta) + ((X^TX)^{-1}X^TZ\\gamma)\\\\ &amp; = \\beta + (X^TX)^{-1}X^TZ\\gamma \\end{align}\\] We are done now, but we cannot say the estimator is unbiased as we don’t have \\(E[\\hat\\beta]= \\beta\\). Instead, we have this extra term of \\((X^TX)^{-1}X^TZ\\gamma\\). This is the bias! So, we failed to adjust for our confounding variable \\(Z\\) and ended up with this bias. Wrapping Up We covered a lot of material in this chapter related to ancestry inference in genetic studies. In the next section we will talk about correlated data, where some of the ideas we talked about here will return. "],["correlated-data.html", " 3 Correlated Data", " 3 Correlated Data "],["glossary.html", "Glossary Unit 1 Unit 2", " Glossary There are many terms that come along with talking about genetic data. The following are some terms to remember as you read this summary. Terms are generally listed in the order which they appear in each unit. Unit 1 SNPs (single nucleotide polymorphisms): a single nucleotide at a specific position in the genome, genetic variant at least 1% frequent in the population Genome: complete set of genes or genetic material Chromosome: condensed DNA structure, all chromosomes together is the genome Genetic variants: positions where DNA sequences differ SNV (single nucleotide variant): a single nucleotide change in the DNA sequence. SNP and SNV are typically used interchangeably. You might see SNP used in instances where the minor allele is not too rare Allele: the different possible nucleotides at some position. The combinations are A - T and G - C Minor allele: at this location, which allele is the least common MAF (minor allele frequency): the frequency of the minor allele, can get very small Loci/locus: means location, can be used to refer to a single SNP or a larger region in the genome Unit 2 Pedigree: how genealogical ancestors are related to each other, like an extensive family tree Genealogical ancestry: the identifiable ancestors in your pedigree Genetic ancestry: the paths through one’s pedigree by which genetic traits have been inherited Genetic similarity: classifying genomes as similar to specific populations Population admixture: the mating of individuals with different geographical/continental origins Principal Component Analysis: a method of dimension reduction to keep the most variation from our original data by weighting the variables together into new components (PCs). Loading: How much of each SNP contributes to a particular PC, the weights. Score: the transformed values for individuals along the new PC axes. Confounding: variables we should adjust for as they are causally related to both our predictor and outcome Directed Acyclic Graph: also known as a causal graph. These graphs show how one believes variables are related to each other Bias: tells us how far off target we are Variance: tells us how much our results differ from attempt to attempt OLS: stands for ordinary least squares. Essentially another name for linear regression "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
